<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[碎片知识]]></title>
    <url>%2F2019%2F05%2F27%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[用utf-8读取文件时出现非法字符： “\ufeff”字符，只需将utf-8改成 UTF-8无BOM格式编码即可。 1sentence = sentence.encode(&apos;utf-8&apos;).decode(&apos;utf-8-sig&apos;) Json 保存文件与读取文件1234567import jsonwith open(input_file, encoding=&apos;utf-8&apos;) as f: dicts = json.load(f)with open(dir+&quot;/final.json&quot;, &apos;w&apos;,encoding = &apos;utf-8&apos;) as f: json.dump(final_json,f,ensure_ascii=False)]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>乱码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础知识碎片]]></title>
    <url>%2F2019%2F05%2F24%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[python语法基础知识 判断字典key是否存在 方法1.使用自带函数 12345#生成一个字典d = &#123;&apos;name&apos;:Tom, &apos;age&apos;:10, &apos;Tel&apos;:110&#125;#打印返回值print d.has_key(&apos;name&apos;)#结果返回True 方法2.使用in方法 12345#生成一个字典d = &#123;&apos;name&apos;:Tom, &apos;age&apos;:10, &apos;Tel&apos;:110&#125;#打印返回值，其中d.keys()是列出字典所有的keyprint ‘name’ in d.keys()print &apos;name&apos; in d]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch里面的torch.nn.Parameter()]]></title>
    <url>%2F2019%2F05%2F23%2Fpytorch-nn-Parameter%2F</url>
    <content type="text"><![CDATA[转载 原文链接 torch.nn.Parameter()官方介绍&emsp;&emsp;Parameters对象是一种会被视为模块参数（module parameter）的Tensor张量。 &emsp;&emsp;Parameters类是Tensor 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 Module类中被使用并被当做这个Module类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个Module类的参数列表(list of parameters)之中，同时也就会被添加入此Module类的 parameters()方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非Parameter的 Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。Parameters: data (Tensor) – 参数张量(parameter tensor). requires_grad (bool, optional) – 参数是否需要梯度， 默认为 True。更多细节请看 如何将子图踢出反向传播过程。+ torch.nn.Parameter() 具体说明&emsp;&emsp;在刷官方Tutorial的时候发现了一个用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size)),看了官方教程里面的解释也是云里雾里，于是在栈溢网看到了一篇解释，并做了几个实验才算完全理解了这个函数。首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。 &emsp;&emsp;在concat注意力机制中，权值V是不断学习的所以要是parameter类型，不直接使用一个torch.nn.Linear()可能是因为学习的效果不好。通过做下面的实验发现，linear里面的weight和bias就是parameter类型，且不能够使用tensor类型替换，还有linear里面的weight甚至可能通过指定一个不同于初始化时候的形状进行模型的更改。]]></content>
      <categories>
        <category>pytorch基础</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker使用]]></title>
    <url>%2F2019%2F05%2F17%2Fdocker-use%2F</url>
    <content type="text"><![CDATA[docker简介&emsp;&emsp;docker 是一个开源的应用容器引擎，Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。相关文档 下载安装 docker国内镜像 官方文档 基本命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162## 展示docker信息docker --versiondocker versiondocker info## 运行docker镜像(加上-it则是交互式运行) ctrl+D退出交互模式docker run hello-worlddocker run -it hello-world docker run -ti --name myhelloword -d hello-world ## 列出所有镜像docker image ls## 删除镜像docker rmi 镜像IDdocker rmi -f 镜像ID## 列出所有容器（容器相当于对象，镜像相当于类） docker container lsdocker container ls --alldocker container ls -aq## 列出所有运行的docker容器docker ps## 进入运行的容器中docker exec -it 容器ID bash## 提交修改docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]-a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像；-m :提交时的说明文字；-p :在commit时，将容器暂停docker commit -m &quot;增加sklearn&quot; b23425436 xiaoduozhou/python_env:3.0##推送到docker hubdocker push [OPTIONS] NAME[:TAG]docker push xiaoduozhou/hello-word:v1docker commit -a &quot;wangshibo&quot; -m &quot;this is test&quot; 651a8541a47d hello-world:v1## 停止容器(停止容器后所有的修改都会消失)docker stop 容器IDdocker kill 容器ID## 重启容器 Docker 容器docker restart 容器ID## 复制文件到docker容器中（或相反） (注意：window系统必须要停止容器后才能复制，然后重新打开容器)docker cp /path/to/file1 容器ID:/path/to/file2docker cp 容器ID:/path/to/file2 /path/to/file1## 文件夹挂载docker run -itv 原路径:环境路径 容器名 /bin/bashdocker run -itv G:\IntentRec\:/user learn_env /bin/bash 常用管理命令1. 开启/停止/重启container（start/stop/restart）&emsp;&emsp;容器可以通过run新建一个来运行，也可以重新start已经停止的container，但start不能够再指定容器启动时运行的指令，因为docker只能有一个前台进程。&emsp;&emsp;容器stop（或Ctrl+D）时，会在保存当前容器的状态之后退出，下次start时保有上次关闭时更改。而且每次进入attach进去的界面是一样的，与第一次run启动或commit提交的时刻相同。 12docker stop $CONTAINER_IDdocker restart $CONTAINER_ID 2.连接到正在运行中的container（attach）&emsp;&emsp;要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上—sig-proxy=false来确保CTRL-D或CTRL-C不会关闭容器。 1docker attach --sig-proxy=false $CONTAINER_ID 3. 删除一个或多个container、image（rm、rmi）&emsp;&emsp;使用过程中会build或commit许多镜像，无用的镜像需要删除。但删除这些镜像是有一些条件的： 同一个IMAGE ID可能会有多个TAG（可能还在不同的仓库），首先你要根据这些 image names 来删除标签，当删除最后一个tag的时候就会自动删除镜像； 承上，如果要删除的多个IMAGE NAME在同一个REPOSITORY，可以通过docker rmi 来同时删除剩下的TAG；若在不同Repo则还是需要手动逐个删除TAG； 还存在由这个镜像启动的container时（即便已经停止），也无法删除镜像； &emsp;&emsp; 删除容器1234docker rm &lt;container_id/contaner_name&gt;删除所有停止的容器docker rm $(docker ps -a -q) &emsp;&emsp; 删除镜像1docker rmi &lt;image_id/image_name ...&gt; 4. docker build 使用此配置生成新的image&emsp;&emsp; build命令可以从Dockerfile和上下文来创建镜像：123docker build [OPTIONS] PATH | URL | -docker build -t my_first_docker -f .\Dockerfile.txt . &emsp;&emsp; 上面的PATH或URL中的文件被称作上下文，build image的过程会先把这些文件传送到docker的服务端来进行的。如果PATH直接就是一个单独的Dockerfile文件则可以不需要上下文；如果URL是一个Git仓库地址，那么创建image的过程中会自动git clone一份到本机的临时目录，它就成为了本次build的上下文。无论指定的PATH是什么，Dockerfile是至关重要的，请参考dockerfile。 dockerfile书写例子：123456789101112131415161718# cat Dockerfile FROM seanlook/nginx:bash_vimEXPOSE 80ENTRYPOINT /usr/sbin/nginx -c /etc/nginx/nginx.conf &amp;&amp; /bin/bash# docker build -t seanlook/nginx:bash_vim_Df .Sending build context to Docker daemon 73.45 MBSending build context to Docker daemon Step 0 : FROM seanlook/nginx:bash_vim ---&gt; aa8516fa0bb7Step 1 : EXPOSE 80 ---&gt; Using cache ---&gt; fece07e2b515Step 2 : ENTRYPOINT /usr/sbin/nginx -c /etc/nginx/nginx.conf &amp;&amp; /bin/bash ---&gt; Running in e08963fd5afb ---&gt; d9bbd13f5066Removing intermediate container e08963fd5afbSuccessfully built d9bbd13f5066 5. 给镜像打上标签（tag）&emsp;&emsp; tag的作用主要有两点：一是为镜像起一个容易理解的名字，二是可以通过docker tag来重新指定镜像的仓库，这样在push时自动提交到仓库12345将同一IMAGE_ID的所有tag，合并为一个新的# docker tag 195eb90b5349 seanlook/ubuntu:rm_test新建一个tag，保留旧的那条记录# docker tag Registry/Repos:Tag New_Registry/New_Repos:New_Tag 6. 查看容器的信息container（ps）&emsp;&emsp;docker ps命令可以查看容器的CONTAINER ID、NAME、IMAGE NAME、端口开启及绑定、容器启动后执行的COMMNAD。经常通过ps来找到CONTAINER_ID。123docker ps 默认显示当前正在运行中的containerdocker ps -a 查看包括已经停止的所有容器docker ps -l 显示最新启动的一个容器（包括已停止的） 7. 查看容器中正在运行的进程（top）&emsp;&emsp;容器运行时不一定有/bin/bash终端来交互执行top命令，查看container中正在运行的进程，况且还不一定有top命令，这是docker top 就很有用了。实际上在host上使用ps -ef|grep docker也可以看到一组类似的进程信息，把container里的进程看成是host上启动docker的子进程就对了 8. 其他命令&emsp;&emsp; docker还有一些如login、cp、logs、export、import、load、kill等不是很常用的命令，比较简单，请参考官网。 相关问题 docker导致VMware启动不了https://blog.minirplus.com/10268/ 注意：遇到下面这个情况时 设置： 然后成功运行 windows修改docker镜像文件存储位置]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周小技巧]]></title>
    <url>%2F2019%2F05%2F12%2Flittleskill%2F</url>
    <content type="text"><![CDATA[pytorch GPU使用 查看gpu使用情况nvidia-smi 使用 kill -9 进程id1kill -9 144953 指定GPU 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot; 命令行指定 1CUDA_VISIBLE_DEVICES=1 python Train.py]]></content>
      <categories>
        <category>pytorch基础</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seq2seq 及 attention机制介绍]]></title>
    <url>%2F2019%2F05%2F09%2Fattention%2F</url>
    <content type="text"><![CDATA[转自：知乎 关于seq2seq实现链接 ： seq2seq &emsp;&emsp;seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。 大体框架 如下： 细节框架如下： 核心公式：输入： $x = (x_{1},…,x_{T_{x}})$输出： $y = (y_{1},…,y_{T_{y}})$ (1) $h_{t} = RNN_{enc}(x_{t},h_{t-1})$，Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。 (2) $s_{t} = RNN_{dec}(y_{t}^{‘},s_{t-1})$，Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。 (3) c_{i}=\sum_{j=1}^{T_{x}}a_{ij}h_{j}，context vector是一个对于encoder输出的hidden states的一个加权平均。 (4) a_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik}) } ，每一个encoder的hidden states对应的权重。 (5) $e_{ij} = score(s_{i},h_{j})$，通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4) (6) $s_{t}^{‘}=tanh(W_{c}[c_{t};s_{t}])$ ，将context vector 和 decoder的hidden states 串起来。 (7) $p(y_{t}|y_{&lt;t},x) = softmax(W_{s}s_{t}^{‘})$，计算最后的输出概率。 Attention介绍&emsp;&emsp;在luong中提到了三种score的计算方法。这里图解前两种： Attention score function: dot &emsp;&emsp;输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。 旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。 对分数做softmax得到一个合为1的权重。 将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 Attention score function: general &emsp;&emsp;输入是encoder的所有hidden states H: 大小为(hid_dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid_dim2, 1）。此处两个hidden state的纬度并不一样。 旋转H为（sequence length, hid dim1) 与 Wa [大小为 (hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数 对分数做softmax得到一个合为1的权重。 将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>神经网络</tag>
        <tag>pytorch，seq2seq</tag>
        <tag>端到端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda基本使用]]></title>
    <url>%2F2019%2F05%2F08%2FcondaUse%2F</url>
    <content type="text"><![CDATA[conda常用命令1.查看已安装环境123conda info -e或conda env list 2.创建新环境mycondaEnv1conda create -n mycondaEnv python=3.5 3.激活conda1conda activate mycondaEnv 4.退出当前环境：1conda deactivate 5.删除conda1conda env remove -n mycondaEnv 6.在conda中安装包1234conda env list 查看环境conda activate mycondaEnv 激活进入环境pip install jieba 安装包conda deactivate 退出环境 7.conda复制环境12conda create -n 新名字 --clone 已存在的conda名如：conda create -n BBB --clone AAA 8.conda导出以及导入已有环境：在新机器上会自动执行创建出一个相同的环境123conda env export &gt; environment.yaml 导出conda env create -f environment.yaml 导入 9.pip导出包以及导入包123pip freeze &gt; requirements.txt 导出pip install -r requirements.txt 导入 conda使用错误情况123conda activate myConda报错CommandNotFoundError: Your shell has not been properly configured to use &apos;conda activate&apos;. 解决办法1234# 激活环境source activate# 退出环境source deactivate]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch基本语法]]></title>
    <url>%2F2019%2F05%2F04%2Fpytorch-base-use%2F</url>
    <content type="text"><![CDATA[pytorch基础操作： 初始化0矩阵12x = torch.zeros(5, 3, dtype=torch.float)x = torch.tensor([5.5, 3]) #构建指定元素的矩阵 加法123x = torch.randn(2,3)y = torch.randn(2,3)z = torch.add(x, y) 任何要原地改动tensor的操作，需要加 _. 比如: x.copy_(y), x.t_(), x.add_(y)will change x.123456x = torch.randn(2,3)print("x",x)y = torch.randn(2,3)print("y",y)x.copy_(y)print("x",x) torch.view(v1,v2,…)来resize或者reshape tensor，例如12x = torch.randn(2,3,2)print("x",x.shape,x.view(-1,3).shape) tensor转numpy,若tensor只有一个元素可以使用.item()取出。123456x = torch.randn(2,3,2)y = x.numpy()z = torch.tensor(1)print("x",type(x))print("y",type(y))print("z",z,z.item()) pytorch拼接 cat沿着数据某一维度拼接123456x = torch.randn(2,3)y = torch.randn(2,3)z = torch.cat((x,y),1)print("x",x,x.shape)print("y",y,y.shape)print("z",z,z.shape) stack 增加新的维度进行堆叠123456x = torch.randn(2,3)y = torch.randn(2,3)z = torch.stack((x,y),1)print("x",x,x.shape)print("y",y,y.shape)print("z",z,z.shape) squeeze 和 unsqueeze, squeeze(dim_n)压缩，即去掉元素数量为1的dim_n维度。同理unsqueeze(dim_n)，增加dim_n维度，元素数量为1。123x = torch.randn(2,1,3)print("x",x.squeeze().shape)print("x",x.unsqueeze(0).shape) 把某一维度的元素反过来(例如第二维)12345x = torch.randn(2,3,2)print("x",x)index = [3-i-1 for i in range(3)]print(index)print("x_change",x[:,index,:]) topk 1torch.topk(input, k, dim=None, largest=True, + + sorted=True, out=None) -&gt; (Tensor, LongTensor) input (Tensor) – 输入张量 k (int) – “top-k”中的k dim (int, optional) – 排序的维 largest (bool, optional) 布尔值，控制返回最大或最小值sorted (bool, optional) 布尔值，控制返回值是否排序 out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffer 12345678output = torch.tensor([[-5.4783, 0.2298 , 0.11], [-4.2573, -0.4794 ,0.05], [-0.1070, -5.1511 , 0.01], [-0.1785, -4.3339,-0.02]])value,index = output.topk(2,dim=1)print(value)print(index) 扩大张量 expand返回张量的一个新视图，可以将张量的 单个维度 扩大为更大的尺寸。张量也可以扩大为更高维，新增加的维度将附在前面。 扩大张量不需要分配新内存，仅仅是新建一个张量的视图。任意一个一维张量在不分配新内存情况下都可以扩展为任意的维度。传入-1则意味着维度扩大不涉及这个维度。123456a = torch.rand((2,3,1))b = a.expand(2,3,4)c = a.expand(-1,-1,4)print(&quot;a&quot;,a)print(&quot;b&quot;,b)print(&quot;c&quot;,c) 沿着某个维度重复 repeat几个数字分别表示原来维度上重复的数字，如果多了就相当于多加一个维度12A = torch.randn([2,2,3])print(A.shape,A.repeat(3,2,2,3).shape) contiguouscontiguous一般与transpose，permute,view搭配使用 即使用transpose或permute进行维度变换后，调用contiguous，然后方可使用view对维度进行变形。 具体原因我还没搞清，看到网上有两种说法，一种是维度变换后tensor在内存中不再是连续存储的，而view操作要求连续存储，所以需要contiguous,另一种是说维度变换后的变量是之前变量的浅复制，指向同一区域，即view操作会连带原来的变量一同变形，这是不合法的，所以也会报错，先甭管是哪个原因吧，记得这样用。1encoder_outputs.contiguous().view(batch_size * max_len, -1)]]></content>
      <categories>
        <category>pytorch基础</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任务型对话系统的用户模拟器]]></title>
    <url>%2F2019%2F05%2F04%2Fusersim%2F</url>
    <content type="text"><![CDATA[原文链接 论文概述&emsp;&emsp;作者总结了前人构造用户模拟器方法，提出了一个比较通用的用户模拟器的框架，并且还有开源的源码，既包含基于规则又有基于模型的。 相关工作介绍&emsp;&emsp;用户模拟器的构造一直是一个争议的问题，没有一个普遍可接受的标准，但是一个好的用户模拟器的重要的特征必须是在整个对话过程中具有连贯的行为。理想情况下，一个好的度量标准应该度量用户模拟器和真实人类行为之间的相关性，但是很难找到一个被广泛接受的度量标准。因此，没有标准的方法来构建用户模拟器。但可以从两方面来总结用户模拟器相关的文献: 从 粒度 层面上来说，用户模拟器可以分为对话行为级别和语言级别 从 方法 层面上来说，用户模拟器分为基于规则的和基于模型的（从训练数据中学习得到） 过去方法&emsp;&emsp;早期的模型有基于bi-gram的,利用$P(a_{u}|a_{m})$基于上一次系统行为去选择用户行为，但是比较不合理，有两点原因：1.这个模型只能看到最后一个系统行为。 2. 如果用户切换意图那么这个模型可能产出不和逻辑的行为，因为它没有把意图考虑进去。关这个方法的后续工作中，有很多工作都试图解决这些问题。问题1 可以通过查看更长的对话历史记录来选择下一个用户动作来解决;问题2 可以通过显式地将用户目标合并到用户状态建模中来解决。 &emsp;&emsp;基于seq2seq的方式通过端到端的方法来模拟用户，这种方法把用户和系统的对话当做 源到端 的序列生成问题，可能更适合类似于chat-bot的类型，不太合适任务型对话，而且需要大量数据 &emsp;&emsp; agenda-based 基于议程的用户模拟器提供一种方便的机制来显式编码对话历史和用户目标。用户目标goal由描述用户请求request和约束的slot-value对组成。对话状态由一个类似堆栈的格式表示，用户行为生成为一系列简单的push和pop操作，确保用户在会话过程中行为的一致性。 本文方法&emsp;&emsp;这篇文章结合seq2seq的方法和基于agenda-based的方法，在对话行为层面用agenda-based的方法，然后用一个seq2seq的方法来将对话行为翻译成自然语言的形式。 用户模拟器介绍&emsp;&emsp;agenda-based的方法，利用栈来表示用户状态（包含对话历史和用户行为),状态更新（包含状态转换和用户行为生成）用栈的pop、push来建模。下面详细描述一个基于规则的用户模拟器。 用户目标&emsp;&emsp;对话的第一步是生成用户目标，用户目标包含两个部分： inform_slots 包含一些 slot value 对作为限制 request_slots 包含一个slot集合 ，这个集合表示用户未知，但是在接下来的对话中想知道的。 &emsp;&emsp;为了让目标更真实，加入一些限制规则：slot将被分为两组。对于买电影票场景，一些元素必须出现在用户目标中，称作 Required slots，包含 moviename, theater, starttime, date, number of people;剩下的元素作为可选项。ticket 是默认出现在request_slots部分的slot。 &emsp;&emsp;从已标注的数据中生成用户目标，主要采用两种方法： 从用户第一轮问句（打招呼的问句除外）中检测所有的槽（已知或未知）组建用户目标，因为第一句往往包含了用户所有的需求。 从所有用户轮问题中提取所有的槽（以第一次出现为准），组建用户目标 &emsp;&emsp;把得到的用户目标存在用户目标数据库中，每次运行一个对话就从中随机选取一个。 用户行为&emsp;&emsp; 第一个用户行为： 这个工作关注用户行为的初始化，当我们随机选取一个用户目标时，为了保证真实性，在生成时需要加一些规则限制。比如，第一轮往往是request 行为，至少包含一个 informable slot，如果该意图知道电影名，那么moviename 必须出现在首轮，等； &emsp;&emsp;用户状态status $s_{u}$被分解为$agenda A$ 和 $goal G$,其中 G 由两部分组成(C,R)分别表示 限制C和请求R。在每一个时间t，用户模拟器将会根据当前状态$s_{u,t}$和上个系统动作$a_{m,t-1}$生成下一个用户行为$a_{u,t}$,然后更新当前用户状态$s^{‘}_{u,t}$。 &emsp;&emsp;没有NLU（自然语言理解）模块下，作者引入噪音模型来模拟NLU模块的失误。有两个级别的错误，意图级别的识别失误和slot级别的失误。slot有3中类型错误： slot deletion：槽未识别 incorrect slot value：槽对槽值错 incorrect slot：槽错 &emsp;&emsp;如果代理动作是inform(taskcomplete)，则表示代理已经收集了所有信息并准备好预订电影票。用户模拟器将检查当前堆栈是否为空，并执行约束检查，以确保代理试图预订正确的电影票。这确保用户以一致的、面向目标的方式进行操作。 对话状态&emsp;&emsp;总共有3种状态：未结束，成功，失败。当agent不是inform(taskcomplete) 或者对话轮数没有超过阈值时状态都是 未结束 。完全满足条件则是超过，否则失败。 &emsp;&emsp;有一个特殊情况，如果识别完全正确但数据库中没有，本文依然判断为失败。 NLU&emsp;&emsp;使用一个LSTM网络joint 的训练intent识别和预测标签。 标签用IOB的方式标记，每句的结尾加上。 NLG&emsp;&emsp;本文采取了一个混合模板和模型的方法来实现NLG Template-based NLG：对于每个对话行为采用预定义的模板 Model-based NLG：输入用户行为，通过一个LSTM decoder生成一个带slot placeholders的句子，后文再用真实的slot value 填充placeholders的值。在 LSTM decoder中采用 beam search（集束搜索）,beam 的大小为3. &emsp;&emsp;如果对话行为能在预定义的规则模板中找到，则使用基于模板的NLG去生成，采用基于模型的方式。 使用&emsp;&emsp;要度量代理的效果，有3个参数：{成功率，平均奖励，平均轮数}，它们都提供了关于代理质量的不同信息。一般而言他们3个是强相关的，一个好的代理同时具有这3方面比较优秀的数字。]]></content>
      <categories>
        <category>对话系统</category>
        <category>用户模拟器</category>
      </categories>
      <tags>
        <tag>对话系统</tag>
        <tag>用户模拟器</tag>
        <tag>自然语言生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分治]]></title>
    <url>%2F2019%2F05%2F01%2F19-5-1-leetcode%2F</url>
    <content type="text"><![CDATA[题目1. 求 Pow(x,n)no.50 题目简介：&emsp;&emsp;求pow(x,n) ,时间复杂度要小。 题解：&emsp;&emsp;首先 n可能有正负，所以分两种情况处理 return 1/ Pow(x,-n,map)和 return Pow(x,n,map); 在计算pow的时候 根据n是奇数或者是偶数来进行分治。用map记录已经计算过的值，减少递归的数量级。 代码：1234567891011121314151617181920212223242526272829303132​class Solution &#123; public double myPow(double x, int n) &#123; Map&lt;Integer,Double&gt; map = new HashMap&lt;&gt;(); if(n &lt;0)&#123; return 1/ Pow(x,-n,map); &#125;else&#123; return Pow(x,n,map); &#125; &#125; double Pow(double x, int n,Map&lt;Integer,Double&gt; map)&#123; if (n ==0)return 1; if(n == 1)return x; if(n == 2)return x*x; if(map.get(n) != null)&#123; //计算过了就直接返回 return map.get(n); &#125;else&#123; Double t=0.0; if(n %2 == 0)&#123; t = Pow(x,n/2,map)*Pow(x,n/2,map); //n为偶数 &#125;else&#123; t = Pow(x,n/2,map)*Pow(x,n/2,map)*x; //n为奇数 &#125; map.put(n,t); //没计算过就存起来 return t; &#125; &#125;&#125;​]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>分治</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test]]></title>
    <url>%2F2019%2F04%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 数学 f(x) = a_1x^n + a_2x^{n-1} + a_3x^{n-2}公式显示错误https://www.jianshu.com/p/49538f7d9512]]></content>
  </entry>
</search>
