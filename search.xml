<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[one]]></title>
    <url>%2F2019%2F05%2F09%2Fone%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[seq2seq 及 attention机制介绍]]></title>
    <url>%2F2019%2F05%2F09%2Fattention%2F</url>
    <content type="text"><![CDATA[转自：知乎 关于seq2seq实现链接 ： seq2seq &emsp;&emsp;seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。 大体框架 如下： 细节框架如下： 核心公式：输入： $x = (x_{1},…,x_{T_{x}})$输出： $y = (y_{1},…,y_{T_{y}})$ (1) $h_{t} = RNN_{enc}(x_{t},h_{t-1})$，Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。 (2) $s_{t} = RNN_{dec}(y_{t}^{‘},s_{t-1})$，Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。 (3) c_{i}=\sum_{j=1}^{T_{x}}a_{ij}h_{j}，context vector是一个对于encoder输出的hidden states的一个加权平均。 (4) a_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik}) } ，每一个encoder的hidden states对应的权重。 (5) $e_{ij} = score(s_{i},h_{j})$，通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4) (6) $s_{t}^{‘}=tanh(W_{c}[c_{t};s_{t}])$ ，将context vector 和 decoder的hidden states 串起来。 (7) $p(y_{t}|y_{&lt;t},x) = softmax(W_{s}s_{t}^{‘})$，计算最后的输出概率。 Attention介绍&emsp;&emsp;在luong中提到了三种score的计算方法。这里图解前两种： Attention score function: dot &emsp;&emsp;输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。 旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。 对分数做softmax得到一个合为1的权重。 将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 Attention score function: general &emsp;&emsp;输入是encoder的所有hidden states H: 大小为(hid_dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid_dim2, 1）。此处两个hidden state的纬度并不一样。 旋转H为（sequence length, hid dim1) 与 Wa [大小为 (hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数 对分数做softmax得到一个合为1的权重。 将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>神经网络</tag>
        <tag>pytorch，seq2seq</tag>
        <tag>端到端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda基本使用]]></title>
    <url>%2F2019%2F05%2F08%2FcondaUse%2F</url>
    <content type="text"><![CDATA[conda常用命令1.查看已安装环境123conda info -e或conda env list 2.创建新环境mycondaEnv1conda create -n mycondaEnv python=3.5 3.激活conda1conda activate mycondaEnv 4.退出当前环境：1conda deactivate 5.删除conda1conda env remove -n mycondaEnv 6.在conda中安装包1234conda env list 查看环境conda activate mycondaEnv 激活进入环境pip install jieba 安装包conda deactivate 退出环境 7.conda复制环境12conda create -n 新名字 --clone 已存在的conda名如：conda create -n BBB --clone AAA 8.conda导出以及导入已有环境：在新机器上会自动执行创建出一个相同的环境123conda env export &gt; environment.yaml 导出conda env create -f environment.yaml 导入 9.pip导出包以及导入包123pip freeze &gt; requirements.txt 导出pip install -r requirements.txt 导入]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch基本语法]]></title>
    <url>%2F2019%2F05%2F04%2Fpytorch-base-use%2F</url>
    <content type="text"><![CDATA[pytorch基础操作： 初始化0矩阵12x = torch.zeros(5, 3, dtype=torch.float)x = torch.tensor([5.5, 3]) #构建指定元素的矩阵 加法123x = torch.randn(2,3)y = torch.randn(2,3)z = torch.add(x, y) 任何要原地改动tensor的操作，需要加 _. 比如: x.copy_(y), x.t_(), x.add_(y)will change x.123456x = torch.randn(2,3)print("x",x)y = torch.randn(2,3)print("y",y)x.copy_(y)print("x",x) torch.view(v1,v2,…)来resize或者reshape tensor，例如12x = torch.randn(2,3,2)print("x",x.shape,x.view(-1,3).shape) tensor转numpy,若tensor只有一个元素可以使用.item()取出。123456x = torch.randn(2,3,2)y = x.numpy()z = torch.tensor(1)print("x",type(x))print("y",type(y))print("z",z,z.item()) pytorch拼接 cat沿着数据某一维度拼接123456x = torch.randn(2,3)y = torch.randn(2,3)z = torch.cat((x,y),1)print("x",x,x.shape)print("y",y,y.shape)print("z",z,z.shape) stack 增加新的维度进行堆叠123456x = torch.randn(2,3)y = torch.randn(2,3)z = torch.stack((x,y),1)print("x",x,x.shape)print("y",y,y.shape)print("z",z,z.shape) squeeze 和 unsqueeze, squeeze(dim_n)压缩，即去掉元素数量为1的dim_n维度。同理unsqueeze(dim_n)，增加dim_n维度，元素数量为1。123x = torch.randn(2,1,3)print("x",x.squeeze().shape)print("x",x.unsqueeze(0).shape) 把某一维度的元素反过来(例如第二维)12345x = torch.randn(2,3,2)print("x",x)index = [3-i-1 for i in range(3)]print(index)print("x_change",x[:,index,:]) topk 1torch.topk(input, k, dim=None, largest=True, + + sorted=True, out=None) -&gt; (Tensor, LongTensor) input (Tensor) – 输入张量 k (int) – “top-k”中的k dim (int, optional) – 排序的维 largest (bool, optional) 布尔值，控制返回最大或最小值sorted (bool, optional) 布尔值，控制返回值是否排序 out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffer 12345678output = torch.tensor([[-5.4783, 0.2298 , 0.11], [-4.2573, -0.4794 ,0.05], [-0.1070, -5.1511 , 0.01], [-0.1785, -4.3339,-0.02]])value,index = output.topk(2,dim=1)print(value)print(index) 扩大张量 expand返回张量的一个新视图，可以将张量的 单个维度 扩大为更大的尺寸。张量也可以扩大为更高维，新增加的维度将附在前面。 扩大张量不需要分配新内存，仅仅是新建一个张量的视图。任意一个一维张量在不分配新内存情况下都可以扩展为任意的维度。传入-1则意味着维度扩大不涉及这个维度。123456a = torch.rand((2,3,1))b = a.expand(2,3,4)c = a.expand(-1,-1,4)print(&quot;a&quot;,a)print(&quot;b&quot;,b)print(&quot;c&quot;,c) 沿着某个维度重复 repeat几个数字分别表示原来维度上重复的数字，如果多了就相当于多加一个维度12A = torch.randn([2,2,3])print(A.shape,A.repeat(3,2,2,3).shape)]]></content>
      <categories>
        <category>pytorch基础</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任务型对话系统的用户模拟器]]></title>
    <url>%2F2019%2F05%2F04%2Fusersim%2F</url>
    <content type="text"><![CDATA[原文链接 论文概述&emsp;&emsp;作者总结了前人构造用户模拟器方法，提出了一个比较通用的用户模拟器的框架，并且还有开源的源码，既包含基于规则又有基于模型的。 相关工作介绍&emsp;&emsp;用户模拟器的构造一直是一个争议的问题，没有一个普遍可接受的标准，但是一个好的用户模拟器的重要的特征必须是在整个对话过程中具有连贯的行为。理想情况下，一个好的度量标准应该度量用户模拟器和真实人类行为之间的相关性，但是很难找到一个被广泛接受的度量标准。因此，没有标准的方法来构建用户模拟器。但可以从两方面来总结用户模拟器相关的文献: 从 粒度 层面上来说，用户模拟器可以分为对话行为级别和语言级别 从 方法 层面上来说，用户模拟器分为基于规则的和基于模型的（从训练数据中学习得到） 过去方法&emsp;&emsp;早期的模型有基于bi-gram的,利用$P(a_{u}|a_{m})$基于上一次系统行为去选择用户行为，但是比较不合理，有两点原因：1.这个模型只能看到最后一个系统行为。 2. 如果用户切换意图那么这个模型可能产出不和逻辑的行为，因为它没有把意图考虑进去。关这个方法的后续工作中，有很多工作都试图解决这些问题。问题1 可以通过查看更长的对话历史记录来选择下一个用户动作来解决;问题2 可以通过显式地将用户目标合并到用户状态建模中来解决。 &emsp;&emsp;基于seq2seq的方式通过端到端的方法来模拟用户，这种方法把用户和系统的对话当做 源到端 的序列生成问题，可能更适合类似于chat-bot的类型，不太合适任务型对话，而且需要大量数据 &emsp;&emsp; agenda-based 基于议程的用户模拟器提供一种方便的机制来显式编码对话历史和用户目标。用户目标goal由描述用户请求request和约束的slot-value对组成。对话状态由一个类似堆栈的格式表示，用户行为生成为一系列简单的push和pop操作，确保用户在会话过程中行为的一致性。 本文方法&emsp;&emsp;这篇文章结合seq2seq的方法和基于agenda-based的方法，在对话行为层面用agenda-based的方法，然后用一个seq2seq的方法来将对话行为翻译成自然语言的形式。 用户模拟器介绍&emsp;&emsp;agenda-based的方法，利用栈来表示用户状态（包含对话历史和用户行为),状态更新（包含状态转换和用户行为生成）用栈的pop、push来建模。下面详细描述一个基于规则的用户模拟器。 用户目标&emsp;&emsp;对话的第一步是生成用户目标，用户目标包含两个部分： inform_slots 包含一些 slot value 对作为限制 request_slots 包含一个slot集合 ，这个集合表示用户未知，但是在接下来的对话中想知道的。 &emsp;&emsp;为了让目标更真实，加入一些限制规则：slot将被分为两组。对于买电影票场景，一些元素必须出现在用户目标中，称作 Required slots，包含 moviename, theater, starttime, date, number of people;剩下的元素作为可选项。ticket 是默认出现在request_slots部分的slot。 &emsp;&emsp;从已标注的数据中生成用户目标，主要采用两种方法： 从用户第一轮问句（打招呼的问句除外）中检测所有的槽（已知或未知）组建用户目标，因为第一句往往包含了用户所有的需求。 从所有用户轮问题中提取所有的槽（以第一次出现为准），组建用户目标 &emsp;&emsp;把得到的用户目标存在用户目标数据库中，每次运行一个对话就从中随机选取一个。 用户行为&emsp;&emsp; 第一个用户行为： 这个工作关注用户行为的初始化，当我们随机选取一个用户目标时，为了保证真实性，在生成时需要加一些规则限制。比如，第一轮往往是request 行为，至少包含一个 informable slot，如果该意图知道电影名，那么moviename 必须出现在首轮，等； &emsp;&emsp;用户状态status $s_{u}$被分解为$agenda A$ 和 $goal G$,其中 G 由两部分组成(C,R)分别表示 限制C和请求R。在每一个时间t，用户模拟器将会根据当前状态$s_{u,t}$和上个系统动作$a_{m,t-1}$生成下一个用户行为$a_{u,t}$,然后更新当前用户状态$s^{‘}_{u,t}$。 &emsp;&emsp;没有NLU（自然语言理解）模块下，作者引入噪音模型来模拟NLU模块的失误。有两个级别的错误，意图级别的识别失误和slot级别的失误。slot有3中类型错误： slot deletion：槽未识别 incorrect slot value：槽对槽值错 incorrect slot：槽错 &emsp;&emsp;如果代理动作是inform(taskcomplete)，则表示代理已经收集了所有信息并准备好预订电影票。用户模拟器将检查当前堆栈是否为空，并执行约束检查，以确保代理试图预订正确的电影票。这确保用户以一致的、面向目标的方式进行操作。 对话状态&emsp;&emsp;总共有3种状态：未结束，成功，失败。当agent不是inform(taskcomplete) 或者对话轮数没有超过阈值时状态都是 未结束 。完全满足条件则是超过，否则失败。 &emsp;&emsp;有一个特殊情况，如果识别完全正确但数据库中没有，本文依然判断为失败。 NLU&emsp;&emsp;使用一个LSTM网络joint 的训练intent识别和预测标签。 标签用IOB的方式标记，每句的结尾加上。 NLG&emsp;&emsp;本文采取了一个混合模板和模型的方法来实现NLG Template-based NLG：对于每个对话行为采用预定义的模板 Model-based NLG：输入用户行为，通过一个LSTM decoder生成一个带slot placeholders的句子，后文再用真实的slot value 填充placeholders的值。在 LSTM decoder中采用 beam search（集束搜索）,beam 的大小为3. &emsp;&emsp;如果对话行为能在预定义的规则模板中找到，则使用基于模板的NLG去生成，采用基于模型的方式。 使用&emsp;&emsp;要度量代理的效果，有3个参数：{成功率，平均奖励，平均轮数}，它们都提供了关于代理质量的不同信息。一般而言他们3个是强相关的，一个好的代理同时具有这3方面比较优秀的数字。]]></content>
      <categories>
        <category>对话系统</category>
        <category>用户模拟器</category>
      </categories>
      <tags>
        <tag>对话系统</tag>
        <tag>用户模拟器</tag>
        <tag>自然语言生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分治]]></title>
    <url>%2F2019%2F05%2F01%2F19-5-1-leetcode%2F</url>
    <content type="text"><![CDATA[题目1. 求 Pow(x,n)no.50 题目简介：&emsp;&emsp;求pow(x,n) ,时间复杂度要小。 题解：&emsp;&emsp;首先 n可能有正负，所以分两种情况处理 return 1/ Pow(x,-n,map)和 return Pow(x,n,map); 在计算pow的时候 根据n是奇数或者是偶数来进行分治。用map记录已经计算过的值，减少递归的数量级。 代码：1234567891011121314151617181920212223242526272829303132​class Solution &#123; public double myPow(double x, int n) &#123; Map&lt;Integer,Double&gt; map = new HashMap&lt;&gt;(); if(n &lt;0)&#123; return 1/ Pow(x,-n,map); &#125;else&#123; return Pow(x,n,map); &#125; &#125; double Pow(double x, int n,Map&lt;Integer,Double&gt; map)&#123; if (n ==0)return 1; if(n == 1)return x; if(n == 2)return x*x; if(map.get(n) != null)&#123; //计算过了就直接返回 return map.get(n); &#125;else&#123; Double t=0.0; if(n %2 == 0)&#123; t = Pow(x,n/2,map)*Pow(x,n/2,map); //n为偶数 &#125;else&#123; t = Pow(x,n/2,map)*Pow(x,n/2,map)*x; //n为奇数 &#125; map.put(n,t); //没计算过就存起来 return t; &#125; &#125;&#125;​]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>分治</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test]]></title>
    <url>%2F2019%2F04%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 数学 f(x) = a_1x^n + a_2x^{n-1} + a_3x^{n-2}公式显示错误https://www.jianshu.com/p/49538f7d9512]]></content>
  </entry>
</search>
